# -*- coding: utf-8 -*-
"""Ai_Therepist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/SmritiVerma6725/Ai-Therepist/blob/main/Ai_Therepist.ipynb
"""

!pip install ipykernel

!pip install langchain_groq

import os
from dotenv import load_dotenv
from langchain_groq import ChatGroq

load_dotenv() ## aloading all the environment variable

groq_api_key=os.getenv("GROQ_API_KEY")

!pip install langchain_groq

import os
os.environ["GROQ_API_KEY"] = "add your key"

!pip install dotenv

model = ChatGroq(
    model="llama3-8b-8192",
    groq_api_key=groq_api_key,
    temperature=0
)
model

result=model.invoke("What is schizophrenia?")
print(result.content)

!pip install pypdf

!pip install langchain

!pip install langchain_community

!pip install langchain-community langchain chromadb sentence-transformers

!pip install chromadb

import os
from langchain_groq import ChatGroq
import gradio as gr

# Check if key loads (print shows in "Logs" tab)
groq_api_key = os.getenv("GROQ_API_KEY")
print("GROQ_API_KEY loaded:", bool(groq_api_key))

import os
from langchain_groq import ChatGroq
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

groq_api_key = os.getenv("GROQ_API_KEY")


def initialize_model():
    model = ChatGroq(
        model_name="llama3-8b-8192",
        groq_api_key=groq_api_key,
        temperature=0
    )
    return model

def create_vector_db():
    loader = DirectoryLoader(
        r"/content/drive/MyDrive/Ai therepist /Data",
        glob="**/*.pdf",
        loader_cls=PyPDFLoader
    )
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='/content/chroma_db')
    vector_db.persist()
    print("ChromaDB created and data saved")
    return vector_db

def setup_qa_chain(vector_db, model):
    retriever = vector_db.as_retriever()
    prompt_templates = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
{context}
User: {question}
Chatbot:"""
    PROMPT = PromptTemplate(template=prompt_templates, input_variables=['context', 'question'])

    qa_chain = RetrievalQA.from_chain_type(
        llm=model,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

def main():
    print("Initializing chatbot...")
    model = initialize_model()

    db_path = "/content/chroma_db"
    if not os.path.exists(db_path):
        vector_db = create_vector_db()
    else:
        embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
        vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

    qa_chain = setup_qa_chain(vector_db, model)

    while True:
        query = input(" \nHuman: ")
        if query.lower() == "exit":
            print("Chatbot: Take care of yourself. Goodbye!")
            break

        response = qa_chain.run(query)
        print(f"Chatbot: {response}")

if __name__ == "__main__":
    main()

create_vector_db()

!pip install gradio

import os
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
import gradio as gr

groq_api_key = os.getenv("GROQ_API_KEY")


def initialize_model():
    model = ChatGroq(
        model_name="llama3-8b-8192",
        groq_api_key=groq_api_key,
        temperature=0
    )
    return model

def create_vector_db():
    # Replace raw string with os.path.join
    loader = DirectoryLoader(
    os.path.join(os.getcwd(), "/content/drive/MyDrive/Ai therepist /Data"),  # Better path handling
    glob="**/*.pdf",
    loader_cls=PyPDFLoader)

    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='/content/chroma_db')
    vector_db.persist()
    print("ChromaDB created and data saved")
    return vector_db

def setup_qa_chain(vector_db, model):
    retriever = vector_db.as_retriever()
    prompt_templates = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
{context}
User: {question}
Chatbot:"""
    PROMPT = PromptTemplate(template=prompt_templates, input_variables=['context', 'question'])
    qa_chain = RetrievalQA.from_chain_type(
        llm=model,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

print("Initializing chatbot...")
model = initialize_model()
db_path = "/content/chroma_db"
if not os.path.exists(db_path):
   vector_db = create_vector_db()

else:
   embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
   vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)


qa_chain = setup_qa_chain(vector_db, model)

def chatbot_response(message, history):
    response = qa_chain.run(message)
    return response






with gr.Blocks(theme ="gradio/soft") as app:
  gr.Markdown("Ai Therepist")
  gr.Markdown("Your Pocket Companion for your anytime mood")

  gr.Markdown("For the Smiles, the Tears, and Everything in Between.")


  chatbot = gr.ChatInterface(
    fn=chatbot_response,
    title="Ai Therepist",
    type="messages"  # important!
)


app.launch(debug=True)



from huggingface_hub import login
login()

##pushing the code from huggingface for parmanent link to the chatbot

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p my-ai-therapist

import os

os.makedirs("/content/my_gradio_app", exist_ok=True)

from huggingface_hub import upload_folder

upload_folder(
    folder_path="/content/drive/MyDrive/Ai therepist /GradioApp",  # replace with your app folder path
    repo_id="SmritiVerma6725/Ai-Therepist",  # your existing space
    repo_type="space"
)

from langchain_community.embeddings import HuggingFaceBgeEmbeddings

